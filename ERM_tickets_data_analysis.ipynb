{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olexandr7/erm_visitation_analysis/blob/main/ERM_tickets_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General idea for now is quite straightforward - we clean up data a bit and then export to parquet files; parquet files get loaded to warehouse in Snowflake;\n",
        "could also do direct Python - Snowflake pipeline but for MVP it should be fine  the way it is;"
      ],
      "metadata": {
        "id": "VtD8lKJHBXhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "N6riWCcGPiwC"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read log file\n",
        "df = pd.read_csv(\n",
        "    'card.log.0',\n",
        "    delimiter=' ',\n",
        "    names=[\"date\", \"timezone\", \"device_id\", \"ticket_info\", \"ticket_id\"],\n",
        "    engine='python'  # safer for odd delimiters\n",
        ")"
      ],
      "metadata": {
        "id": "DjJT1wLlPlJF"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Clean and combine timestamp strings\n",
        "df['timestamp_str'] = df['date'].str.strip('[') + ' ' + df['timezone'].str.strip(']')\n",
        "\n",
        "# Step 3: Parse into timezone-aware datetime\n",
        "df['timestamp'] = pd.to_datetime(\n",
        "    df['timestamp_str'],\n",
        "    format='%d/%b/%Y:%H:%M:%S %z',\n",
        "    errors='coerce'  # safely handle bad rows\n",
        ")\n",
        "\n",
        "# Step 4: Drop temporary columns (optional)\n",
        "df = df.drop(columns=['date', 'timezone', 'timestamp_str'])"
      ],
      "metadata": {
        "id": "-ud48PVlPtM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse into two separate dataframes based on timezone\n",
        "df_utc3 = df[df['timestamp'].astype(str).str.contains(r'\\+03:00$', regex=True)].copy()\n",
        "df_utc2 = df[df['timestamp'].astype(str).str.contains(r'\\+02:00$', regex=True)].copy()"
      ],
      "metadata": {
        "id": "mu0aMCrPRhYu"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing timezone-specific conversions\n",
        "df_utc2['timestamp'] = pd.to_datetime(df_utc2['timestamp'], errors='coerce')\n",
        "df_utc2['timestamp'] = df_utc2['timestamp'].dt.tz_localize(None)\n",
        "\n",
        "df_utc3['timestamp'] = pd.to_datetime(df_utc3['timestamp'], errors='coerce')\n",
        "df_utc3['timestamp'] = df_utc3['timestamp'].dt.tz_localize(None)"
      ],
      "metadata": {
        "id": "_csdfo3XSFpX"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge both subsets\n",
        "df_merged = pd.concat([df_utc3, df_utc2], ignore_index=True)\n",
        "\n",
        "# Optional: sort chronologically\n",
        "df_merged = df_merged.sort_values(by='timestamp').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "H-6twkSDS-Xl"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(df) == len(df_merged), \"Row count mismatch after splitting and merging!\"\n",
        "print(\"Original row count:\", len(df))\n",
        "print(\"Merged row count:  \", len(df_merged))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng4C3gv4P3kN",
        "outputId": "b66e20d4-a8fe-4aa9-d707-13cbd98904d0"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original row count: 95490\n",
            "Merged row count:   95490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.to_parquet('card_log_0.parquet', index=False)"
      ],
      "metadata": {
        "id": "1vy45PM8TlQz"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined into function to process all the files at once\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_file(filename):\n",
        "    try:\n",
        "        print(f\"Processing {filename} ...\")\n",
        "        df = pd.read_csv(filename, delimiter=' ', names=[\"date\", \"timezone\", \"device_id\", \"ticket_info\", \"ticket_id\"])\n",
        "\n",
        "        # Clean and combine timestamp strings\n",
        "        df['timestamp_raw'] = df['date'].astype(str).str.strip('[') + df['timezone'].astype(str).str.strip(']')\n",
        "\n",
        "        # Split by timezone offsets using regex on raw string\n",
        "        df_utc3 = df[df['timestamp_raw'].str.contains(r'\\+0300$', regex=True)].copy()\n",
        "        df_utc2 = df[df['timestamp_raw'].str.contains(r'\\+0200$', regex=True)].copy()\n",
        "\n",
        "        # Parse timestamps\n",
        "        df_utc3['timestamp'] = pd.to_datetime(df_utc3['timestamp_raw'], format='%d/%b/%Y:%H:%M:%S%z', errors='coerce')\n",
        "        df_utc2['timestamp'] = pd.to_datetime(df_utc2['timestamp_raw'], format='%d/%b/%Y:%H:%M:%S%z', errors='coerce')\n",
        "\n",
        "        # Remove timezone info (local naive time)\n",
        "        df_utc3['timestamp'] = df_utc3['timestamp'].dt.tz_localize(None)\n",
        "        df_utc2['timestamp'] = df_utc2['timestamp'].dt.tz_localize(None)\n",
        "\n",
        "        # Combine splits\n",
        "        df_processed = pd.concat([df_utc3, df_utc2], ignore_index=True)\n",
        "\n",
        "        print(f\"  -> {len(df_processed)} rows processed successfully.\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame on error\n",
        "\n",
        "all_dfs = []\n",
        "total_files = 13  # files card.log.0 to card.log.12 inclusive\n",
        "\n",
        "for i in range(total_files):\n",
        "    filename = f'card.log.{i}'\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Warning: {filename} does not exist, skipping.\")\n",
        "        continue\n",
        "\n",
        "    df_processed = process_file(filename)\n",
        "    if not df_processed.empty:\n",
        "        all_dfs.append(df_processed)\n",
        "\n",
        "if not all_dfs:\n",
        "    print(\"No data processed. Exiting.\")\n",
        "else:\n",
        "    df_final = pd.concat(all_dfs, ignore_index=True).sort_values('timestamp').reset_index(drop=True)\n",
        "    print(f\"Combined DataFrame has {len(df_final)} rows.\")\n",
        "\n",
        "    output_file = 'card_all_combined.parquet'\n",
        "    df_final.to_parquet(output_file, index=False, compression='snappy')\n",
        "    print(f\"Data saved to {output_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_-IvlHUt5M",
        "outputId": "bba1cb20-e09d-4198-9ffd-e83eeb2b8b98"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing card.log.0 ...\n",
            "  -> 95490 rows processed successfully.\n",
            "Processing card.log.1 ...\n",
            "  -> 107301 rows processed successfully.\n",
            "Processing card.log.2 ...\n",
            "  -> 243449 rows processed successfully.\n",
            "Processing card.log.3 ...\n",
            "  -> 222553 rows processed successfully.\n",
            "Processing card.log.4 ...\n",
            "  -> 138620 rows processed successfully.\n",
            "Processing card.log.5 ...\n",
            "  -> 86765 rows processed successfully.\n",
            "Processing card.log.6 ...\n",
            "  -> 63678 rows processed successfully.\n",
            "Processing card.log.7 ...\n",
            "  -> 60960 rows processed successfully.\n",
            "Processing card.log.8 ...\n",
            "  -> 43602 rows processed successfully.\n",
            "Processing card.log.9 ...\n",
            "  -> 32556 rows processed successfully.\n",
            "Processing card.log.10 ...\n",
            "  -> 41538 rows processed successfully.\n",
            "Processing card.log.11 ...\n",
            "  -> 8254 rows processed successfully.\n",
            "Processing card.log.12 ...\n",
            "  -> 67592 rows processed successfully.\n",
            "Combined DataFrame has 1212358 rows.\n",
            "Data saved to card_all_combined.parquet.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO+FR5CbQvZtxMVtkIqJVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}